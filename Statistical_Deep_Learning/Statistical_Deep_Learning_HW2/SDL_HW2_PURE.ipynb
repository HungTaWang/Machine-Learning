{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistical Deep Learning HW2 (Appendix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 王泓達 R13546017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data_utils\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import tensorflow as tf\n",
    "import lightning as L\n",
    "from lightning.pytorch import LightningModule, LightningDataModule, Trainer\n",
    "from lightning.pytorch.callbacks import EarlyStopping\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from lightning.pytorch.callbacks import Callback\n",
    "import os\n",
    "import math\n",
    "import kagglehub\n",
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1\n",
    "Consider a 2-feature linear regression problem in which $x_1$ always lies in the range $[8, 9]$ and $x_2$ always lies in the range $[8000, 9000]$. $\\\\$ Comment on why gradient descent is likely to perform more efficiently after feature normalization by inspecting the \n",
    "objective function $\\\\$ over a toy training set with three examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Generating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numbers = 3\n",
    "np.random.seed(10)\n",
    "x1 = np.random.normal(8, 9, numbers)\n",
    "x2 = np.random.normal(8000, 9000, numbers)\n",
    "\n",
    "features = np.vstack((np.ones(numbers), x1, x2)).T\n",
    "target_o =   2 * x1 + 3 * x2 + np.random.normal(-9000,9000,numbers)\n",
    "target =   target_o.reshape((3,1))\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(x1, label=\"x1\")\n",
    "plt.plot(x2, label=\"x2\")\n",
    "plt.plot(target, label=\"target\")\n",
    "plt.xlabel(\"Sample Index\")\n",
    "plt.ylabel(\"Value\")\n",
    "plt.title(\"Toy training set with three examples.\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Steepest Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "\n",
    "for i in range(3):\n",
    "    features_Normalization = (features - features.mean())/features.std()\n",
    "    target_Normalization = (target - target.mean())/target.std()\n",
    "\n",
    "def gradient_descent(X, y, w=np.zeros((3, 1)), learning_rate=0.001, iterations=10000):\n",
    "    weights = [w]\n",
    "    counter = 0\n",
    "    for i in range(iterations):\n",
    "        predictions = X.dot(w)\n",
    "        gradient = (1/3) * (X.transpose().dot(predictions - y))\n",
    "        tmp_w = w - learning_rate * gradient\n",
    "        if np.linalg.norm(weights[-1] - tmp_w) < 1e-6 or np.linalg.norm(weights[-1] - tmp_w) > 1e6:\n",
    "            break\n",
    "        w = tmp_w\n",
    "        counter += 1\n",
    "        # print(w) \n",
    "    return w, counter\n",
    "\n",
    "w_original, history = gradient_descent(features, target)\n",
    "w_Normalization, history_Normalization = gradient_descent(features_Normalization, target_Normalization)\n",
    "print(f\"Iterations without Normalization: {history}\")\n",
    "print(f\"Iterations with Normalization: {history_Normalization}\")\n",
    "\n",
    "res1 = (features[0].dot(w_original))[0]-target_o[0]\n",
    "res2 = (features[1].dot(w_original))[0]-target_o[1]\n",
    "res3 = (features[2].dot(w_original))[0]-target_o[2]\n",
    "MSE_w = (res1**2 + res2**2 + res3**2)/3\n",
    "\n",
    "res1 = (features_Normalization[0].dot(w_Normalization))[0]-target_Normalization.reshape((3))[0]\n",
    "res2 = (features_Normalization[1].dot(w_Normalization))[0]-target_Normalization.reshape((3))[1]\n",
    "res3 = (features_Normalization[2].dot(w_Normalization))[0]-target_Normalization.reshape((3))[2]\n",
    "MSE_n = (res1**2 + res2**2 + res3**2)/3\n",
    "\n",
    "print(f\"MSE without Normalization: {MSE_w}\")\n",
    "print(f\"MSE with Normalization: {MSE_n}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison of Accuracy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(target, label='Targets')\n",
    "plt.plot(features.dot(w_original), label='With outNormalization')\n",
    "plt.xlabel(\"Sample Index\")\n",
    "plt.ylabel(\"Value\")\n",
    "plt.title(\"Toy training set with three examples.\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(target_Normalization, label='Targets')\n",
    "plt.plot(features_Normalization.dot(w_StandardScaler), label='With StandardScaler')\n",
    "plt.xlabel(\"Sample Index\")\n",
    "plt.ylabel(\"Value\")\n",
    "plt.title(\"Toy training set with three examples.\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"satvshr/top-4-used-car-sales-datasets-combined\")\n",
    "# read the data as a Pandas dataframe\n",
    "df = pd.read_csv(path+\"/output.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.a. 'feature': [Numbers, Rate]\n",
    "Summarize the missing rates for each variable. That is, for each predictor $x$, what is the proportion of samples with missing values?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variable_is_na = {}\n",
    "\n",
    "for i in df.columns:\n",
    "    variable_is_na[i] = [0,0]\n",
    "    for j in df[i]:\n",
    "        if(pd.isna(j)):\n",
    "            variable_is_na[i][0] += 1\n",
    "    variable_is_na[i][1] += variable_is_na[i][0] / len(df[i])\n",
    "\n",
    "variable_is_na"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.b. Drop col-na(%) > 0.2 and row-na\n",
    "Remove variables with missing rate greater than $0.2$ and samples with missing variables. How many samples are left?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop = []\n",
    "\n",
    "for i in variable_is_na.keys():\n",
    "    if variable_is_na[i][1] > 0.2:\n",
    "        columns_to_drop.append(i)\n",
    "\n",
    "print(f\"columns_to_drop: {columns_to_drop}\")\n",
    "\n",
    "df.drop(columns=columns_to_drop, inplace=True)\n",
    "df = df.dropna(axis=0)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Result: We have 29352 samples left"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.d. Data Transformation\n",
    "Transform the categorical variables into one-hot encoding. $\\\\$\n",
    "Standardize the numerical variables. Use the training dataset to fit the standardization model. $\\\\$\n",
    "Apply the same model to the validation and testing datasets.$\\\\$ \n",
    "Summarize the transformed datasets.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drop the element that rarely apeears"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brand = []\n",
    "for i in df[\"brand\"]:\n",
    "    if i not in brand:\n",
    "        brand.append(i)\n",
    "model = []\n",
    "for i in df[\"model\"]:\n",
    "    if i not in model:\n",
    "        model.append(i)\n",
    "transmission = []\n",
    "for i in df[\"transmission\"]:\n",
    "    if i not in transmission:\n",
    "        transmission.append(i)\n",
    "fuel = []\n",
    "for i in df[\"fuel\"]:\n",
    "    if i not in fuel:\n",
    "        fuel.append(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Selected main features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brand_times = {}\n",
    "for i in brand:\n",
    "    brand_times[i] = 0\n",
    "for i in df[\"brand\"]:\n",
    "    brand_times[i] += 1 / 29352 \n",
    "\n",
    "model_times = {}\n",
    "for i in model:\n",
    "    model_times[i] = 0\n",
    "for i in df[\"model\"]:\n",
    "    model_times[i] += 1 / 29352 \n",
    "\n",
    "transmission_times = {}\n",
    "for i in transmission:\n",
    "    transmission_times[i] = 0\n",
    "for i in df[\"transmission\"]:\n",
    "    transmission_times[i] += 1 / 29352 \n",
    "\n",
    "fuel_times = {}\n",
    "for i in fuel:\n",
    "    fuel_times[i] = 0\n",
    "for i in df[\"fuel\"]:\n",
    "    fuel_times[i] += 1 / 29352 \n",
    "\n",
    "brand_final = []\n",
    "for i in brand_times:\n",
    "    if brand_times[i] > 0.01:\n",
    "        brand_final.append(i)\n",
    "print(brand_final)\n",
    "model_final = []\n",
    "for i in model_times:\n",
    "    if model_times[i] > 0.01:\n",
    "        model_final.append(i)\n",
    "print(model_final)\n",
    "transmission_final = []\n",
    "for i in transmission_times:\n",
    "    if transmission_times[i] > 0.01:\n",
    "        transmission_final.append(i)\n",
    "print(transmission_final)\n",
    "fuel_final = []\n",
    "for i in fuel_times:\n",
    "    if fuel_times[i] > 0.01:\n",
    "        fuel_final.append(i)\n",
    "print(fuel_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### New interested dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_e = df.copy()\n",
    "\n",
    "for i in [\"brand\", \"model\", \"fuel\", \"transmission\"]:\n",
    "    for j in df_e.index:\n",
    "        if df_e[i][j] not in (brand_final + fuel_final + model_final + transmission_final):\n",
    "            df_e[i][j] = np.nan\n",
    "        \n",
    "df_e = df_e.dropna(axis=0)\n",
    "df_e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_number = len(df_e)\n",
    "features_age = np.zeros((19502,1))\n",
    "features_km = np.zeros((19502,1))\n",
    "features_seats = np.zeros((19502,1))\n",
    "target_e = np.zeros((data_number, 1))\n",
    "counter_e = 0\n",
    "\n",
    "for i in df_e.index:\n",
    "    features_age[counter_e,0] = float(df_e[\"age\"][i])\n",
    "    features_km[counter_e,0] = float(df_e[\"km\"][i])\n",
    "    features_seats[counter_e,0] = float(df_e[\"seats\"][i])\n",
    "    target_e[counter_e,0] = float(df_e[\"price\"][i])\n",
    "    counter_e += 1\n",
    "\n",
    "df_e.drop(columns=\"age\", inplace=True)\n",
    "df_e.drop(columns=\"km\", inplace=True)\n",
    "df_e.drop(columns=\"seats\", inplace=True)\n",
    "df_e.drop(columns=\"price\", inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Standardize the numerical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "features_age_standardized = scaler.fit_transform(features_age)\n",
    "features_km_standardized = scaler.fit_transform(features_km)\n",
    "features_seats_standardized = scaler.fit_transform(features_seats)\n",
    "target_e_standardized = scaler.fit_transform(target_e)\n",
    "\n",
    "features_num = np.hstack((features_age_standardized,features_km_standardized,target_e_standardized))\n",
    "features_num"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transform the categorical variables into one-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_cate = pd.get_dummies(df_e)\n",
    "features_cat = features_cate.to_numpy()\n",
    "features_cat = np.array(features_cat, dtype=np.float32)\n",
    "\n",
    "features_cate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_e = np.hstack((features_num, features_cat))\n",
    "data_columns = len(features_e[0])\n",
    "features_e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.c. Split the entire dataset into training, validation, and testing datasets with a 70-20-10 ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_train_e = features_e[0:math.ceil(data_number*0.7)]\n",
    "feature_val_e = features_e[math.ceil(data_number*0.7):math.ceil(data_number*0.9)]\n",
    "feature_test_e = features_e[math.ceil(data_number*0.9):data_number]\n",
    "\n",
    "target_train_e = target_e_standardized[0:math.ceil(data_number*0.7)]\n",
    "target_val_e = target_e_standardized[math.ceil(data_number*0.7):math.ceil(data_number*0.9)]\n",
    "target_test_e = target_e_standardized[math.ceil(data_number*0.9):data_number]\n",
    "\n",
    "print(f\"Numbers of feature_train : {len(feature_train_e)}\")\n",
    "print(f\"Numbers of feature_validation : {len(feature_val_e)}\")\n",
    "print(f\"Numbers of feature_test : {len(feature_test_e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.e. Build the Model with Less than 100 Trainable Parameters.\n",
    "Design a neural network model to predict the price of used cars. The model should have less than 100 trainable parameters. $\\\\$ \n",
    "Summarize the model architecture.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class datasetSetUp(Dataset):\n",
    "    def __init__(self, features, targets):\n",
    "        self.features = features\n",
    "        self.targets = targets\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.features[idx]\n",
    "        y = self.targets[idx]\n",
    "        return x, y\n",
    "\n",
    "class dataModule(LightningDataModule):\n",
    "    def __init__(self, feature_train_e, target_train_e, feature_val_e, target_val_e, batch_size=10):\n",
    "        super().__init__()\n",
    "        self.feature_train_e = torch.tensor(feature_train_e, dtype=torch.float32)\n",
    "        self.target_train_e = torch.tensor(target_train_e, dtype=torch.float32)\n",
    "        self.feature_val_e = torch.tensor(feature_val_e, dtype=torch.float32)\n",
    "        self.target_val_e = torch.tensor(target_val_e, dtype=torch.float32)\n",
    "        self.feature_test_e = torch.tensor(feature_test_e, dtype=torch.float32)\n",
    "        self.target_test_e = torch.tensor(target_test_e, dtype=torch.float32)\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        self.train_dataset_e = datasetSetUp(self.feature_train_e, self.target_train_e)\n",
    "        self.val_dataset_e = datasetSetUp(self.feature_val_e, self.target_val_e)\n",
    "        self.test_dataset_e = datasetSetUp(self.feature_test_e, self.target_test_e)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_dataset_e, batch_size=self.batch_size, shuffle=True)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_dataset_e, batch_size=self.batch_size, shuffle=False)\n",
    "    \n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.test_dataset_e, batch_size=self.batch_size, shuffle=False)\n",
    "\n",
    "class neuralNetwork(LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer_1 = torch.nn.Linear(46, 2)\n",
    "        self.layer_2 = torch.nn.Linear(2, 1)\n",
    "        self.train_losses = []\n",
    "        self.val_losses = []  \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.layer_1(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.layer_2(x)\n",
    "        return x\n",
    "\n",
    "    def MSE(self, logits, labels):\n",
    "        return F.mse_loss(logits, labels)\n",
    "\n",
    "    def training_step(self, train_batch, batch_idx):\n",
    "        x, y = train_batch\n",
    "        logits = self.forward(x)\n",
    "        loss = self.MSE(logits, y)\n",
    "        self.log('train_loss', loss)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, val_batch, batch_idx):\n",
    "        x, y = val_batch\n",
    "        logits = self.forward(x)\n",
    "        loss = self.MSE(logits, y)\n",
    "        self.log('val_loss', loss)\n",
    "\n",
    "    def on_train_epoch_end(self):\n",
    "        avg_train_loss = self.trainer.callback_metrics['train_loss'].item()\n",
    "        self.train_losses.append(avg_train_loss)\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        avg_val_loss = self.trainer.callback_metrics['val_loss'].item()\n",
    "        self.val_losses.append(avg_val_loss)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=1e-3)\n",
    "\n",
    "data_module = dataModule(feature_train_e, target_train_e, feature_val_e, target_val_e, batch_size=10)\n",
    "model = neuralNetwork()\n",
    "\n",
    "trainer = Trainer(callbacks=[EarlyStopping(monitor=\"val_loss\", mode=\"min\")])\n",
    "trainer.fit(model, data_module)\n",
    "torch.save(model.state_dict(), 'model_weights.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the model in 2.e. (epochs $< 100$) \n",
    "Train the model for at most 100 epochs. $\\\\$\n",
    "Plot the training and validation loss curves (𝑥-axis: epoch, 𝑦-axis: training/validation loss). $\\\\$ \n",
    "Is there any sign of overfitting/underfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(model.train_losses, label='Train Loss')\n",
    "plt.plot(model.val_losses, label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comments: There is not any sign of overfitting or underfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.g. Change early stopping strategy of model in 2.e. \n",
    "Use the early stopping strategy: stop training if the validation loss increases by $1%$ in less than $10$ epochs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class datasetSetUp(Dataset):\n",
    "    def __init__(self, features, targets):\n",
    "        self.features = features\n",
    "        self.targets = targets\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.features[idx]\n",
    "        y = self.targets[idx]\n",
    "        return x, y\n",
    "\n",
    "class dataModule(LightningDataModule):\n",
    "    def __init__(self, feature_train_e, target_train_e, feature_val_e, target_val_e, batch_size=10):\n",
    "        super().__init__()\n",
    "        self.feature_train_e = torch.tensor(feature_train_e, dtype=torch.float32)\n",
    "        self.target_train_e = torch.tensor(target_train_e, dtype=torch.float32)\n",
    "        self.feature_val_e = torch.tensor(feature_val_e, dtype=torch.float32)\n",
    "        self.target_val_e = torch.tensor(target_val_e, dtype=torch.float32)\n",
    "        self.feature_test_e = torch.tensor(feature_test_e, dtype=torch.float32)\n",
    "        self.target_test_e = torch.tensor(target_test_e, dtype=torch.float32)\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        self.train_dataset_e = datasetSetUp(self.feature_train_e, self.target_train_e)\n",
    "        self.val_dataset_e = datasetSetUp(self.feature_val_e, self.target_val_e)\n",
    "        self.test_dataset_e = datasetSetUp(self.feature_test_e, self.target_test_e)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_dataset_e, batch_size=self.batch_size, shuffle=True)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_dataset_e, batch_size=self.batch_size, shuffle=False)\n",
    "    \n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.test_dataset_e, batch_size=self.batch_size, shuffle=False)\n",
    "\n",
    "class neuralNetwork(LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer_1 = torch.nn.Linear(46, 2)\n",
    "        self.layer_2 = torch.nn.Linear(2, 1)\n",
    "        self.train_losses = []\n",
    "        self.val_losses = []  \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.layer_1(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.layer_2(x)\n",
    "        return x\n",
    "\n",
    "    def MSE(self, logits, labels):\n",
    "        return F.mse_loss(logits, labels)\n",
    "\n",
    "    def training_step(self, train_batch, batch_idx):\n",
    "        x, y = train_batch\n",
    "        logits = self.forward(x)\n",
    "        loss = self.MSE(logits, y)\n",
    "        self.log('train_loss', loss)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, val_batch, batch_idx):\n",
    "        x, y = val_batch\n",
    "        logits = self.forward(x)\n",
    "        loss = self.MSE(logits, y)\n",
    "        self.log('val_loss', loss)\n",
    "\n",
    "    def on_train_epoch_end(self):\n",
    "        avg_train_loss = self.trainer.callback_metrics['train_loss'].item()\n",
    "        self.train_losses.append(avg_train_loss)\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        avg_val_loss = self.trainer.callback_metrics['val_loss'].item()\n",
    "        self.val_losses.append(avg_val_loss)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=1e-3)\n",
    "\n",
    "class earlyStoppingStrtegy(Callback):\n",
    "    def __init__(self, max_val_epoch=10, rate=0.01):\n",
    "        super().__init__()\n",
    "        self.max_val_epoch = max_val_epoch\n",
    "        self.rate = rate\n",
    "        self.loss_up = -999\n",
    "        self.loss_up_up = -999\n",
    "        self.loss_now = 999\n",
    "        self.loss_past = 999\n",
    "        self.counter = 0\n",
    "\n",
    "    def on_validation_epoch_end(self, trainer, pl_module):\n",
    "        current_loss = trainer.callback_metrics[\"val_loss\"].item()\n",
    "        self.loss_past = self.loss_now \n",
    "        self.loss_now = current_loss\n",
    "        if self.loss_now > self.loss_past and self.counter == 0:\n",
    "            self.loss_up = self.loss_now\n",
    "            self.counter+1\n",
    "        elif self.loss_now > self.loss_past and self.counter != 0:\n",
    "            self.loss_up_up = self.loss_now\n",
    "            self.counter+1\n",
    "            if self.loss_up_up > self.loss_up*self.rate and self.counter <= self.max_val_epoch:\n",
    "                trainer.should_stop = True\n",
    "        else:\n",
    "            self.loss_up = -999\n",
    "            self.loss_up_up = -999\n",
    "            self.loss_now = 999\n",
    "            self.loss_past = 999\n",
    "\n",
    "data_module2 = dataModule(feature_train_e, target_train_e, feature_val_e, target_val_e, batch_size=10)\n",
    "model2 = neuralNetwork()\n",
    "early_stopping = earlyStoppingStrtegy(max_val_epoch=10, rate=0.01)\n",
    "trainer2 = Trainer(callbacks=[early_stopping], max_epochs=100)\n",
    "\n",
    "trainer2.fit(model2, data_module2)\n",
    "torch.save(model2.state_dict(), 'model2_weights.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot the loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(model2.train_losses, label='Train Loss')\n",
    "plt.plot(model2.val_losses, label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.h.\n",
    "Evaluate the models from (f) and (g) on the testing dataset. Report the mean squared error. $\\\\$\n",
    "Run a linear regression model on the same dataset and report the mean squared error.$\\\\$ \n",
    "Comment on the results.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compare MSE between (f) and (g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('model_weights.pth'))\n",
    "model2.load_state_dict(torch.load('model2_weights.pth'))\n",
    "model.eval()\n",
    "model2.eval()\n",
    "\n",
    "test_loader = data_module.test_dataloader()\n",
    "features_for_testing = torch.tensor(feature_test_e, dtype = torch.float32)\n",
    "target_for_testing = torch.tensor(target_test_e, dtype = torch.float32)\n",
    "\n",
    "with torch.no_grad():  \n",
    "    prediction = model(features_for_testing)  \n",
    "    prediction2 = model2(features_for_testing)   \n",
    "\n",
    "model_residual = (prediction-target_for_testing).numpy()\n",
    "model_residual2 = (prediction2-target_for_testing).numpy()\n",
    "residuals_model1 = []\n",
    "residuals_model2 = []\n",
    "MSE1_total = 0\n",
    "MSE2_total = 0\n",
    "\n",
    "for i in model_residual:\n",
    "    residuals_model1.append(i[0])\n",
    "\n",
    "for i in model_residual2:\n",
    "    residuals_model2.append(i[0])\n",
    "\n",
    "for i in residuals_model1:\n",
    "    MSE1_total += i**2\n",
    "\n",
    "for i in residuals_model2:\n",
    "    MSE2_total += i**2\n",
    "\n",
    "MSE1 = MSE1_total/len(model_residual)\n",
    "MSE2 = MSE2_total/len(model_residual2)\n",
    "\n",
    "print(\"MSE of model in (f): \", MSE1)\n",
    "print(\"MSE of model in (g)\", MSE2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_number = len(feature_test_e)\n",
    "var_number = len(feature_test_e[0])\n",
    "\n",
    "reg = LinearRegression().fit(feature_train_e, target_train_e)\n",
    "predict_lr = np.zeros((1,predict_number))\n",
    "\n",
    "for i in range(predict_number):\n",
    "    predict_tmp = 0\n",
    "    for j in range(var_number):\n",
    "        predict_tmp += reg.coef_[0][j]*feature_test_e[i][j]\n",
    "    predict_tmp += reg.intercept_\n",
    "    predict_lr[0,i] = predict_tmp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MSE of linear regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MSE_3 = 0.0\n",
    "for i in range(predict_number):\n",
    "    MSE_3 = (predict_lr[0,i]-target_test_e[i])**2\n",
    "\n",
    "MSE_3 = MSE_3 / predict_number\n",
    "\n",
    "print(\"MSE of linear regression model: \", MSE_3[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MSE comparison of all methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"MSE of model in (f): \", MSE1)\n",
    "print(\"MSE of model in (g)\", MSE2)\n",
    "print(\"MSE of linear regression model: \", MSE_3[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.i. Draw the residual plots for the three models\n",
    "\n",
    "$x$-axis: observed price; $y$: residual = predicted - observed $\\\\$\n",
    "neural network from (f), neural network from (g), and linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "observation = target_test_e.reshape((1,1950))\n",
    "residuals_model3 = predict_lr[0] - observation\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5), sharey=True)\n",
    "\n",
    "axes[0].scatter(observation, residuals_model1, color='firebrick', alpha=0.5)\n",
    "axes[0].set_title('Residual Plot: Model 1')\n",
    "axes[0].set_xlabel('Predicted Values')\n",
    "axes[0].set_ylabel('Residuals')\n",
    "\n",
    "axes[1].scatter(observation, residuals_model2, color='dodgerblue', alpha=0.5)\n",
    "axes[1].set_title('Residual Plot: Model 2')\n",
    "axes[1].set_xlabel('Predicted Values')\n",
    "\n",
    "axes[2].scatter(observation, residuals_model3, color='mediumaquamarine', alpha=0.5)\n",
    "axes[2].set_title('Residual Plot: Model 3')\n",
    "axes[2].set_xlabel('Predicted Values')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start tensorboard.\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir=lightning_logs/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
